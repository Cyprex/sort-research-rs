# 10~17x faster than what? A performance analysis of Intel' x86-simd-sort (AVX-512)

**WIP: Please do not publish.**

Author: Lukas Bergdoll @Voultapher  
Date: TODO (DD-MM-YYYY)

This is a limited performance analysis of the recently popularized [[1](https://www.phoronix.com/news/Intel-AVX-512-Quicksort-Numpy)] Intel AVX-512 sort implementation.

Bias disclaimer. The author of this analysis is the author of the ipn family of sort implementations.

The words sort implementation and sort algorithm, are expressly *not* used interchangeably. Practically all modern implementations are hybrids, using multiple sort algorithms. As such, the words 'sort algorithm' will only be used to talk about the algorithmic nature of specific building blocks.

Graphs with logarithmic axis are marked as such, these are primarily useful to examine the change of a property, *not* it's absolute values.

## Benchmarks

### Benchmark setup

Benchmarking is notoriously tricky, and especially synthetic benchmarks may not be representative. An incomplete list of relevant factors:

- Input size
- Input type (price to move and price to compare)
- Input pattern (already sorted, random, cardinality, streaks, mixed etc.)
- Hardware prediction and cache effects

```
rustc 1.69.0-nightly (0416b1a6f 2023-02-14)
clang version 15.0.1
Microsoft (R) C/C++ Optimizing Compiler Version 19.31.31104 for x86
Intel Core i9-10980XE 18-Core Processor (Skylake micro-architecture)
```

Modern sort implementations are adaptive, they will try to exploit existing patterns in the data to do less work. A breakdown of the benchmark patterns:

- `ascending`, numbers `0..size`
- `random`, random numbers generated by rand `StdRng::gen` [[2](https://github.com/rust-random/rand)]
- `random_d20`, uniform random numbers in the range `0..=20`
- `random_p5`, 95% 0 and 5% random data, not uniform
- `random_z1`, Zipfian distribution with characterizing exponent s == 1.0 [[3](https://en.wikipedia.org/wiki/Zipf%27s_law)]
- `saws_long`, `(size as f64).log2().round()` number of randomly selected ascending and descending streaks
- `saws_short`, randomly selected ascending and descending streaks in the range of `20..70`

### Contestants

A selection of high-performance in-place sort implementations.

#### Generic comparison based

```
- rust_std_unstable          | `slice::sort_unstable` https://github.com/rust-lang/rust (1)
- rust_ipn_unstable          | https://github.com/Voultapher/sort-research-rs (2)
- cpp_std_msvc_unstable      | MSVC `std::sort` (3)
- cpp_pdqsort_unstable       | https://github.com/orlp/pdqsort (3)
- c_crumsort_unstable        | https://github.com/scandum/crumsort (4) (5)
```

#### Manually vectorized 

```
- cpp_vqsort                 | https://github.com/google/highway/tree/master/hwy/contrib/sort (6)
- cpp_intel_avx512           | https://github.com/intel/x86-simd-sort (6)
```

1. Vendored ca. mid 2022.
2. Still WIP and these are only preliminary results.
3. Build with msvc.
4. Compiled with `#define cmp(a, b) (*(a) > *(b))`. This is required to be competitive, the regular way of providing a comparison function is problematic because of C language limitations.
5. c_crumsort_unstable does an initial analysis and switches to quadsort a merge sort that uses up to N memory making it not in-place ⚠️, it can fall back to in-place merging if the allocation fails. These tests were performed with allocations possible.
6. Build with clang and `-march=native`. Compiled with static dispatch, this would not be portable. Any CPU without AVX-512 support would fail to run the binary. It's unknown what the overhead of dynamic dispatch would be.

### Results `u64`

#### hot-u64-10k

A good benchmark to shine light into the ability of the sort to exploit instruction-level parallelism (ILP) is hot-u64-10000. The input are 10k `u64` values, which fits into the core private L2 data cache for the used Zen3 test machine. The upper limit should be in the order of 4-5 instructions per cycle for such a dataset. 10k elements is enough to reliably exploit existing patterns in the input data. This can be reproduced by running `cargo bench hot-u64-<pattern>-10000`

<img src="assets/hot-u64-10k.png" width=600 />

Using this data to write a headline like:

> Rust std sort is 10x faster than C++ std sort

Is not an honest representation. And it's generally very difficult to compress so much information into a single number, while still remaining representative. Looking at this one input size, on one specific micro-architecture, using this specific set of compilers, and testing these synthetic patterns, yields:

- cpp_intel_avx512 is generally faster than cpp_vqsort at this input size.
- The two manually vectorized sort implementations cpp_intel_avx512 and cpp_vqsort, are not good at exploiting existing patterns in the input.
- cpp_intel_avx512 struggles if there is one very common value in the input (random_p5).
- cpp_std_msvc_unstable is generally the slowest of the comparison based sort implementations.
- rust_std_unstable which is based on cpp_pdqsort_unstable performs similar, with the exception of fully ascending inputs.
- ascending shows the largest variance with the fastest sort being ~33x faster han the slowest.
- More natural random distributions like random_z1 close the gap between manually vectorized code and pdqsort derived designs, rust_std_unstable, rust_ipn_unstable and c_crumsort_unstable.

Whether these patterns are representative will depend on your workload. These are fundamentally synthetic benchmarks exploring sort performance in isolation. Especially small sizes are likely not representative of real world performance, where CPU branch, instruction and data caches may be cold. **These numbers should be interpreted as best case performance under laboratory conditions.**

#### hot-u64-scaling

Measuring random pattern performance across different sizes:
<img src="assets/hot-u64-scaling-random.png" width=600 />

Zoomed in:

<img src="assets/hot-u64-scaling-random_zoomed.png" width=600 />

Observations:

- cpp_vqsort is exceedingly slow for small inputs. This seems to be caused in part by always allocating regardless of input size.
- cpp_intel_avx512 is fast across all sizes, when benchmarked in a hot loop.
- cpp_intel_avx512 and c_crumsort_unstable, differentiate themselves from the other implementations using insertion sort for such inputs.
- cpp_std_msvc_unstable shows sub-log scaling.
- Starting at ~50k cpp_vqsort is the fastest.
- rust_ipn_unstable catches up to cpp_intel_avx512 at ~1m, despite only using SSE2 instructions and using no hardware specific code.

Hot benchmarks are deceiving for small sizes. Instrumenting the Rust standard library and building a custom version of rustc that logs the input size and time spent in `slice::sort`, which is the stable sort of the Rust standard library, yielded these insights clean compiling 60 crates:

Out of the ~500k calls to `slice::sort` 99+% were `len <= 20` and together they account for ~50% of the time.

Running a sort implementation several million times in a loop, with new unique inputs of the same size and pattern is not representative of real world performance effects. To simulate a program that calls sort occasionally with varying sizes, and a cold CPU prediction state, the benchmarks are also run in a mode where between each sort call, the CPU prediction state is trashed [[4](https://github.com/Voultapher/sort-research-rs/blob/b7bcd199e861d6f8b265164242f3c34d5c36c75f/benches/trash_prediction.rs#L7)]. Looking at that important `len <= 20` range:

<img src="assets/cold-u64-scaling-random.png" width=600 />

Zoomed in:

<img src="assets/cold-u64-scaling-random_zoomed.png" width=600 />

Observations:

- rust_ipn_unstable leads the chart, in contrast to the hot results.
- cpp_intel_avx512 trails the chart, in contrast to the hot results.
- Peak throughput drops by 5x compared to hot results.

Another aspect that should be mentioned, is frequency scaling. With the exception of the latest generations of Intel and AMD  micro-architectures, Golden Cove and Zen4, previous Intel AVX-512 implementations scaled down the CPU boost frequency when certain AVX-512 instructions were executed [[5](https://travisdowns.github.io/blog/2020/08/19/icl-avx512-freq.html)]. This may affect real world usage and performance. Another aspect potentially affected by cold code is the micro-op cache, which may introduce additional latency before the longer and more complex AVX-512 instructions can be executed.

The shown cold and hot benchmarks, model the two extremes of the likely range of possible results.

Measuring a more natural zipfian distribution random_z1 across different sizes:
<img src="assets/cold-u64-scaling-random_z1_zoomed.png" width=600 />

Observations:

- Generally similar to random pattern performance scaling.
- cpp_intel_avx512 shows less uniform scaling, with overall worse performance compared to random. 
- The pdqsort derived implementations see a performance uplift compared to random.

### Results `i32`

#### hot-i32-10k

The main difference between `i32` and `u64` is that `i32` is only 4 bytes compared to the 8 bytes of `u64`. In theory this should allow higher bandwidth from the CPUs memory subsystems. Better cache utilization and potentially higher ALU throughput.

<img src="assets/hot-i32-10k.png" width=600 />

Observations:
- Compared to `u64` cpp_intel_avx512 manages to cut it's runtime in half.
- cpp_vqsort only sees a ~1.43x speedup compared to `u64` at this input size.
- All comparison based sort implementations only see a small change compared to `u64`.

#### hot-i32-scaling

Hot:

<img src="assets/hot-i32-scaling-random_zoomed.png" width=600 />

Cold:

<img src="assets/cold-i32-scaling-random_zoomed.png" width=600 />

Observations:

- Very similar scaling to `u64`.
- It takes a bit longer, but eventually cpp_vqsort overtakes cpp_intel_avx512.
- The manually vectorized implementations are a lot better at leveraging the increased potential throughput, than the generic comparison based implementations.

### Direct comparison

A comprehensive look at performance for a single type can be achieved by comparing two implementations and plotting their symmetric relative speedup and slowdown on the Y-axis and the test size on the X-axis. Each line representing a pattern. Eg. a-vs-b, 1% means a is 1.01x faster than b, 100% means a is 2x faster than b, and 200% means a is 3x faster than b, and 300% means 4x. The same works in reverse where -1% means b is 1.01x faster than a, -100% means b 2x a, and -200% means b 3x a. The graphs are fixed to the Y-range -200,200 to allow comparison between graphs.

Comparing the two manually vectorized implementations:

<img src="assets/cpp_intel_avx512-vs-cpp_vqsort-cold-u64.png" width=600 />

Observations:

- Below 10k cpp_intel_avx512 is faster in every pattern than cpp_vqsort.
- Above 100k cpp_vqsort is faster than cpp_intel_avx512 in nearly every pattern.
- For large random inputs cpp_vqsort is ~1.4x faster than cpp_intel_avx512.
- random_5p triggers bad partitions in cpp_intel_avx512 which does not safe-guard against it, making cpp_vqsort is ~17x faster than cpp_intel_avx512 for `len == 10m` for the random_p5 pattern. This seems to indicate that cpp_intel_avx512 has a worse than `O(N * log(N))` worst-case runtime, likely caused by poor pivot selection and no mitigation strategies.

<img src="assets/cpp_intel_avx512-vs-cpp_vqsort-cold-i32.png" width=600 />

Observations:

- `i32` performs very similar to `u64` but the lead cpp_vqsort has for larger inputs shrinks. Random showing only ~1.3x faster performance compared to cpp_vqsort.

Compared to the fastest generic comparison based implementation rust_ipn_unstable:

<img src="assets/cpp_intel_avx512-vs-rust_ipn_unstable-cold-u64.png" width=600 />

Observations:

- Between 36 and 1k elements, cpp_intel_avx512 is faster in nearly every pattern. Between 1k and 100k cpp_intel_avx512 is faster for fully random and saw like patterns. Whereas rust_ipn_unstable is faster for low- cardinality and zipfian patterns, where it can leverage the pdqsort derived ability to filter out common values.

## Author's conclusion and opinion

cpp_intel_avx512 is a very fast sort implementation, especially for random inputs, and it shows nice scaling across different input sizes. This analysis did not look into the overhead of runtime dispatch, which would be required to use it in code that is not uniquely compiled for a specific platform. Looking at some scenarios:

#### Specialized sort in a library

cpp_intel_avx512 can be a great choice, if the input is assumed mostly random, not very small and not of low-cardinality. When inputs of varying sizes have to be sorted and the appropriate hardware supporting AVX-512 is available. At the same time it can be problematic to use it as the sole replacement for calls to sort because of cold performance and potential frequency scaling issues.

#### Specialized HPC code

If you need the best possible throughput, and know you only have large inputs with high-cardinality, cpp_vqsort is the faster option.

#### Language standard library

A standard library implementation should be good in most scenarios, with bad performance in as few scenarios as possible. There are many glaring issues with cpp_intel_avx512 for such a use case. It would only support a small fraction of devices, would increase binary size for all x86 targets. It is not good at exploiting existing patterns in the input. It can't support user-defined comparisons, which would potentially mean that `v.sort()` and `v.sort_by(|a, b| a.cmp(b))` could have a surprising difference in performance. It can't support user-defined types without additional code by the user, eg. `#[derive(Copy, Clone)]struct X(i32)` could have a surprising difference in performance to just `i32`. On top of that, there are issues with cold performance and frequency scaling.

All the results are a snapshot of the current state of the respective implementations. Since making the measurements, there have been changes to c_crumsort_unstable and rust_ipn_unstable, and the people behind cpp_vqsort are looking into small input performance.

## Thanks

I want to thank Jan Wassenberg for his detailed feedback and help.
